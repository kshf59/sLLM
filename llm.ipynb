{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3065fbc5-ae6f-419e-8e47-229b70988bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import bitsandbytes as bnb\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training)\n",
    "\n",
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044a6d7b-ca64-4aa6-8891-f53564760b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "wandb_host = os.getenv(\"WANDB_HOST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c269ab-29af-4da2-b390-3f54afa2c30e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magair08\u001b[0m (\u001b[33mblue_analytics\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login(host = wandb_host)\n",
    "use_wandb = True\n",
    "wandb_run_name = 'Single_GPU_Optim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4243261c-5e4d-4cbb-9ba8-ccbb3b5c8359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user1/projects/llm_ksh/wandb/run-20241016_023002-gabd8k87</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://192.168.100.133:8080/blue_analytics/llm_ksh/runs/gabd8k87' target=\"_blank\">Single_GPU_Optim</a></strong> to <a href='http://192.168.100.133:8080/blue_analytics/llm_ksh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://192.168.100.133:8080/blue_analytics/llm_ksh' target=\"_blank\">http://192.168.100.133:8080/blue_analytics/llm_ksh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://192.168.100.133:8080/blue_analytics/llm_ksh/runs/gabd8k87' target=\"_blank\">http://192.168.100.133:8080/blue_analytics/llm_ksh/runs/gabd8k87</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_wandb = wandb.init(project=\"llm_ksh\", name = wandb_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc4b96-eda8-4c89-a6f5-2df3763bda1e",
   "metadata": {},
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install datasets -U\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install wandb\n",
    "!pip install pandas\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3d2d63-4c17-4e62-a519-c1532d8c8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/user1/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "my_hf_key = api_key\n",
    "login(my_hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f31e14c-ecdb-4acd-a71d-3379d2385c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "data_path = 'DopeorNope/Ko-Optimize_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd957cdd-5a41-4897-99d8-3b827ccc09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d319b83b-b0eb-4f35-978a-f0896cc76f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7794c99f-bd8b-439d-8588-d87bc4599e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>∫[0,t] (GCP × SM × TA) dt = SA\\n\\n여기서\\n∫ = 적분 ...</td>\n",
       "      <td>예, 주어진 공식을 다음과 같이 수정할 수 있습니다:\\n\\n∫[0,T] [(GCP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>귀하는 사람들이 정보를 찾도록 도와주는 AI 어시스턴트입니다. 사용자가 질문을 합니...</td>\n",
       "      <td>정답 -&gt; 문장 A\\n질문 -&gt; 다음 두 문장 중 상식에 어긋나는 것은 어느 것입니...</td>\n",
       "      <td>질문 -&gt; 다음 두 문장 중 상식에 어긋나는 것은 어느 것입니까?\\n옵션:\\n- 문...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>이산화탄소 배출을 줄이는 5가지 방법을 나열하세요.</td>\n",
       "      <td>1. 재생 에너지원 사용: 태양열, 풍력, 수력, 지열 등 이산화탄소를 배출하지 않...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>귀하는 사람들이 정보를 찾도록 도와주는 AI 어시스턴트입니다. 사용자가 질문을 합니...</td>\n",
       "      <td>많은 정치인들이 소농에 대해 이야기하는 것을 좋아하지만, 실제로 거의 모든 농장은 ...</td>\n",
       "      <td>서서히 해봅시다: 노화된 록스타의 손이 협조하지 않는 것은 건강 문제를 나타냅니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>귀하는 항상 설명을 제공하는 도움이 되는 조수입니다. 5살짜리 아이에게 대답한다고 ...</td>\n",
       "      <td>다음 리뷰의 감상을 알려주세요: 솔직히 저는 사춘기 욕구 때문에 이 영화를 봤어요....</td>\n",
       "      <td>이 리뷰에는 긍정적인 감정이 담겨 있습니다. 영화를 보고 유머와 연기를 즐기며 즐거...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0                                                      \n",
       "1  귀하는 사람들이 정보를 찾도록 도와주는 AI 어시스턴트입니다. 사용자가 질문을 합니...   \n",
       "2                                                      \n",
       "3  귀하는 사람들이 정보를 찾도록 도와주는 AI 어시스턴트입니다. 사용자가 질문을 합니...   \n",
       "4  귀하는 항상 설명을 제공하는 도움이 되는 조수입니다. 5살짜리 아이에게 대답한다고 ...   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  ∫[0,t] (GCP × SM × TA) dt = SA\\n\\n여기서\\n∫ = 적분 ...   \n",
       "1  정답 -> 문장 A\\n질문 -> 다음 두 문장 중 상식에 어긋나는 것은 어느 것입니...   \n",
       "2                       이산화탄소 배출을 줄이는 5가지 방법을 나열하세요.   \n",
       "3  많은 정치인들이 소농에 대해 이야기하는 것을 좋아하지만, 실제로 거의 모든 농장은 ...   \n",
       "4  다음 리뷰의 감상을 알려주세요: 솔직히 저는 사춘기 욕구 때문에 이 영화를 봤어요....   \n",
       "\n",
       "                                              output  \n",
       "0  예, 주어진 공식을 다음과 같이 수정할 수 있습니다:\\n\\n∫[0,T] [(GCP ...  \n",
       "1  질문 -> 다음 두 문장 중 상식에 어긋나는 것은 어느 것입니까?\\n옵션:\\n- 문...  \n",
       "2  1. 재생 에너지원 사용: 태양열, 풍력, 수력, 지열 등 이산화탄소를 배출하지 않...  \n",
       "3  서서히 해봅시다: 노화된 록스타의 손이 협조하지 않는 것은 건강 문제를 나타냅니다....  \n",
       "4  이 리뷰에는 긍정적인 감정이 담겨 있습니다. 영화를 보고 유머와 연기를 즐기며 즐거...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15fab9f-a544-420e-a5ab-2381a429b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbcbd36-51a4-4ee3-a61c-d3a33b8d5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 세팅 : QLoRA시 pad 토큰을 eos로 설정하기\n",
    "bos = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\" : \"<|reserved_special_token_0>\"})\n",
    "\n",
    "tokenizer.pad_token_id = eos\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06986c76-d8e0-4eff-9793-08392577bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_len = 4098\n",
    "val_size = 0.005\n",
    "train_on_inputs = False\n",
    "add_eos_token = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "134a7a0c-68b0-4003-91e2-2c9bce824725",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"prompt_input\": \"아래는 문제를 설명하는 지시사항과, 구체적인 답변을 방식을 요구하는 입력이 함께 있는 문장입니다. 이 요청에 대해 적절하게 답변해주세요.\\n###입력:{input}\\n###지시사항:{instruction}\\n###답변:\",\n",
    "    \"prompt_no_input\": \"아래는 문제를 설명하는 지시사항입니다. 이 요청에 대해 적절하게 답변해주세요.\\n###지시사항:{instruction}\\n###답변\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee468c1-e626-4c76-8710-d7f2896360b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(\n",
    "    instruction: str,\n",
    "    input: Union[None, str] = None,\n",
    "    label: Union[None, str] = None,\n",
    "    verbose: bool = False\n",
    ") -> str:\n",
    "    if input:\n",
    "        res = template[\"prompt_input\"].format(instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    if verbose:\n",
    "        print(res)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1deac4d5-0a6b-4d32-a4f5-a28c40b7f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(prompt, truncation=True, max_length=cut_off_len, padding=False, return_tensors=None,)\n",
    "    if (result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cut_off_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a70c7c06-4af2-4b2c-8c13-fd3ff6a42697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"input\"],\n",
    "        data_point[\"output\"]\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    if not train_on_inputs:\n",
    "        user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"input\"])\n",
    "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=add_eos_token)\n",
    "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "        if add_eos_token:\n",
    "            user_prompt_len -= 1\n",
    "\n",
    "        tokenized_full_prompt[\"labels\"] = [-100] * user_prompt_len + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417d2af8-2d97-4cf7-94a5-0f032f35b039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef03419da36a46e5ba24e89a41993208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792ae27a1a0040168932e307537961dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if val_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(test_size = val_size, shuffle=True, seed=42)\n",
    "    train_data = (train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt))\n",
    "    val_data = (train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt))\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417d157-598f-4c01-9382-7f059ca8b7e7",
   "metadata": {},
   "source": [
    "#### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ca5b43c-ae08-4ede-8771-ce9ce5f32429",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_storage=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ecc43f7-1fce-460a-95d3-5a8d16b7e67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d0b79b80564039b0d50c6d9c37b866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = quantization_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = {\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9bd7d66-9d99-44b4-b012-2a2877966968",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cd872ae-664b-4ce8-99a3-51fb31c1fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r = 16,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = ['q_proj','k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de1b7e72-3f3b-4e65-9ba3-e080b7867c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32d8178e-79d2-44e5-b29a-dcaffa992ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable target module: ['up_proj', 'q_proj', 'gate_proj', 'o_proj', 'v_proj', 'down_proj', 'k_proj']\n"
     ]
    }
   ],
   "source": [
    "print('Trainable target module:', find_all_linear_names(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54704606-a48f-4cdc-981e-89d6b3ecbf63",
   "metadata": {},
   "source": [
    "#### QLoRA Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf927bf7-20d5-4976-82d1-9ecba9830d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "083b44d1-95cb-4ca6-a1bf-141898e642c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "967ff2aa-7c0c-40d9-b0fd-bb45f24fe33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13631488 || all params: 2809401344 || trainable%: 0.4852097059436731\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b59445-8d6f-445e-b9aa-e91b21b93e92",
   "metadata": {},
   "source": [
    "#### Hyper parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5ea0be4-eac2-4554-b4e4-485758e6ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './llama_singleGPU-v1'\n",
    "num_epochs = 1\n",
    "micro_batch_size = 1\n",
    "gradient_accumulation_steps = 8\n",
    "warmup_steps = 10\n",
    "#warmup_steps = 100\n",
    "learning_rate = 5e-8 \n",
    "group_by_length = False\n",
    "optimizer = 'paged_adamw_8bit'\n",
    "\n",
    "# adam 활용시\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "\n",
    "lr_scheduler = 'cosine'\n",
    "#lr_scheduler = 'cosine', 'linear', 'constant'\n",
    "logging_steps = 1\n",
    "\n",
    "use_fp16 = False\n",
    "use_bf_16 = True\n",
    "evaluation_strategy = 'steps'\n",
    "eval_steps = 50\n",
    "save_steps = 50\n",
    "save_strategy = 'steps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed8afac0-b951-460b-8bec-e3431f7015d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6ab73eb-39b1-42d2-ad10-4fa63d78908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/anaconda3/envs/ksh_llm/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = val_data,\n",
    "    args = TrainingArguments(\n",
    "    per_device_train_batch_size = micro_batch_size,\n",
    "    per_device_eval_batch_size = micro_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    warmup_steps = warmup_steps,\n",
    "    num_train_epochs = num_epochs,\n",
    "    learning_rate = learning_rate,\n",
    "    adam_beta1 = beta1,\n",
    "    adam_beta2 = beta2,\n",
    "    fp16 = use_fp16,\n",
    "    bf16 = use_bf_16,\n",
    "    logging_steps = logging_steps,\n",
    "    optim = optimizer,\n",
    "    evaluation_strategy = evaluation_strategy if val_size > 0 else \"no\",\n",
    "    save_strategy = 'steps',\n",
    "    eval_steps = eval_steps,\n",
    "    save_steps = save_steps,\n",
    "    lr_scheduler_type = lr_scheduler,\n",
    "    output_dir = output_dir,\n",
    "    #save_total_limit = 4,\n",
    "    load_best_model_at_end = True if val_size > 0 else False,\n",
    "    group_by_length = group_by_length,\n",
    "    report_to = \"wandb\" if use_wandb else None,\n",
    "    run_name = wandb_run_name if use_wandb else None,\n",
    "    ),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors='pt', padding=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f989757-e727-4125-8fab-a20c25e07ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/anaconda3/envs/ksh_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='1243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 201/1243 15:30 < 1:21:14, 0.21 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.754700</td>\n",
       "      <td>2.032317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.688500</td>\n",
       "      <td>2.033165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.543500</td>\n",
       "      <td>2.031544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.649000</td>\n",
       "      <td>2.031247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/anaconda3/envs/ksh_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/user1/anaconda3/envs/ksh_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/user1/anaconda3/envs/ksh_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/user1/anaconda3/envs/ksh_llm/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ec79f-4157-4a24-a471-b0e80ef08031",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9bfa4a-3419-42fe-921e-50f37459d17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ksh_llm",
   "language": "python",
   "name": "ksh_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
